{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ilNdcHYpxA"
      },
      "source": [
        "## Muhammad Abdullah\n",
        "\n",
        "## 20i-0867\n",
        "\n",
        "## CS-A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPFX-zYYYurC"
      },
      "source": [
        "Data Reading And Saving in a Data Frame\n",
        "\n",
        "Data Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJJUl60zYh1W",
        "outputId": "b51dadba-8a66-4c46-c9d6-255e4d1bedac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values:\n",
            " timestamp_(min)       0\n",
            "feature_0             0\n",
            "feature_1             0\n",
            "feature_2             0\n",
            "feature_3             0\n",
            "feature_4             0\n",
            "feature_5            47\n",
            "feature_6           204\n",
            "feature_7            48\n",
            "feature_8            48\n",
            "feature_9             1\n",
            "feature_10           48\n",
            "feature_11            1\n",
            "feature_12            1\n",
            "feature_13           48\n",
            "feature_14            1\n",
            "feature_15            1\n",
            "feature_16            1\n",
            "feature_17            1\n",
            "feature_18            1\n",
            "feature_19            1\n",
            "feature_20            1\n",
            "feature_21         2137\n",
            "feature_22          205\n",
            "feature_23            1\n",
            "feature_24           48\n",
            "dtype: int64\n",
            "[[-1.48864889  0.07261161  0.06629247 ... -0.92171602 -0.67108887\n",
            "  -1.1773965 ]\n",
            " [ 0.13372782 -1.52243094  0.1619146  ...  1.20754299 -1.03976419\n",
            "  -0.93468764]\n",
            " [-1.37268208  0.09506607  0.15474992 ... -0.92510057 -0.17952179\n",
            "  -0.44926992]\n",
            " ...\n",
            " [-2.09903705  1.00389759 -1.29323688 ... -0.8835458   0.43493707\n",
            "   0.84517734]\n",
            " [-0.62137987 -1.60506465  0.58629473 ...  2.38405073 -1.16265596\n",
            "   0.11705076]\n",
            " [ 1.15361916  0.30432435  0.45259416 ...  0.67729657 -0.91687242\n",
            "  -0.04475515]]\n",
            "Below is X_Test\n",
            "[[-0.6338634  -1.60446412  0.57438161 ...  2.42184489 -1.03976419\n",
            "   0.19795371]\n",
            " [ 0.64792646 -1.41132028 -0.47351584 ...  0.6498441  -0.5481971\n",
            "   0.44066257]\n",
            " [-0.02855955 -0.13466658 -1.10334811 ...  0.81399485 -1.03976419\n",
            "  -1.25829945]\n",
            " ...\n",
            " [ 0.48964771 -1.48309316 -0.61117012 ...  0.76078218 -0.30241356\n",
            "   1.0878862 ]\n",
            " [-0.50279926  1.21137554 -0.46980368 ... -1.11745592  0.55782884\n",
            "  -0.28746401]\n",
            " [ 0.24150604  0.7525243   0.1712355  ... -0.72296763  0.80361239\n",
            "  -1.50100831]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"training.csv\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "# Option 1: Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Option 2: Impute missing values (using mean, median, etc.)\n",
        "data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = data.drop(columns=[\"timestamp_(min)\"])  # Features\n",
        "y = data[\"timestamp_(min)\"]  # Target (if available)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "print(X_train)\n",
        "\n",
        "print(\"Below is X_Test\")\n",
        "\n",
        "print(X_test)\n",
        "\n",
        "# Now X_train, X_test are preprocessed and scaled feature sets ready for model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMlVHniR7PV0",
        "outputId": "bef69075-1599-4eb7-fe84-691344158f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values:\n",
            " timestamp_(min)    0\n",
            "feature_0          0\n",
            "feature_1          0\n",
            "feature_2          0\n",
            "feature_3          0\n",
            "feature_4          0\n",
            "feature_5          0\n",
            "feature_6          0\n",
            "feature_7          0\n",
            "feature_8          0\n",
            "feature_9          0\n",
            "feature_10         0\n",
            "feature_11         0\n",
            "feature_12         0\n",
            "feature_13         1\n",
            "feature_14         1\n",
            "feature_15         1\n",
            "feature_16         1\n",
            "feature_17         1\n",
            "feature_18         1\n",
            "feature_19         1\n",
            "feature_20         1\n",
            "feature_21         1\n",
            "feature_22         1\n",
            "feature_23         1\n",
            "feature_24         1\n",
            "dtype: int64\n",
            "[[-4.44153233 -5.08046141  1.7294737  ...  1.13783775 -0.212661\n",
            "  -0.40757212]\n",
            " [ 0.04375253 -0.54308551 -0.9827567  ... -1.19855171 -0.58381463\n",
            "  -1.1177379 ]\n",
            " [ 1.03376867  0.03006936 -0.19832565 ... -1.36629675  0.52964626\n",
            "   0.30259366]\n",
            " ...\n",
            " [-1.98078306 -2.87927278 -1.62537132 ... -1.16616529 -0.212661\n",
            "   0.2011414 ]\n",
            " [-0.85141481  0.90803344 -0.42358251 ...  0.07240762 -0.212661\n",
            "   0.91130719]\n",
            " [ 0.8993603   1.13711046  0.55140851 ... -0.85724868 -0.212661\n",
            "  -0.50902438]]\n",
            "Below is Y_Test\n",
            "[[ 1.40121477  0.2159379  -0.04022361 ...  0.46021422  0.52964626\n",
            "   0.2011414 ]\n",
            " [-0.56456347 -0.64361947 -1.05935698 ...  0.77618941 -1.69727552\n",
            "  -1.62499918]\n",
            " [-0.59895704 -1.16230719 -1.02127685 ...  0.53702816  0.52964626\n",
            "   0.50549817]\n",
            " ...\n",
            " [ 1.19409833 -0.04607258  0.42861548 ...  1.0211636  -0.95496826\n",
            "   0.50549817]\n",
            " [-0.48344826  0.0543565   1.62173835 ...  1.34170611  1.64310715\n",
            "  -0.00176311]\n",
            " [ 1.39053468  0.1669875  -0.04354009 ...  0.92815645  0.52964626\n",
            "   0.80985493]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "# Option 1: Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Option 2: Impute missing values (using mean, median, etc.)\n",
        "data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = data.drop(columns=[\"timestamp_(min)\"])  # Features\n",
        "y = data[\"timestamp_(min)\"]  # Target (if available)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "Y_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "Y_train, Y_test = train_test_split(Y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "print(Y_train)\n",
        "\n",
        "print(\"Below is Y_Test\")\n",
        "\n",
        "print(Y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnGKH5U7dohN"
      },
      "source": [
        "Step 2: Data Augnemntation with Geometric Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5xxuHMcd-gQ",
        "outputId": "23164def-fcba-46b1-a6e3-ceace90c6214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_augmented: (276327, 25)\n",
            "Shape of Y_train_augmented_trimmed: (41292, 25)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define function to apply geometric transformations\n",
        "def apply_geometric_transformations(data):\n",
        "    augmented_data = []\n",
        "    num_instances = data.shape[0]\n",
        "\n",
        "    for i in range(num_instances):\n",
        "        instance = data[i]\n",
        "\n",
        "        # Apply rotation\n",
        "        rotation_angle = np.random.uniform(-15, 15)  # Random rotation angle between -15 and 15 degrees\n",
        "        rotated_instance = rotate_instance(instance, angle=rotation_angle)\n",
        "\n",
        "        # Apply translation\n",
        "        translation_shift = np.random.uniform(-0.1, 0.1, size=instance.shape)  # Random translation shift for each dimension\n",
        "        translated_instance = instance + translation_shift\n",
        "\n",
        "        # Apply scaling\n",
        "        scaling_factor = np.random.uniform(0.9, 1.1)  # Random scaling factor between 0.9 and 1.1\n",
        "        scaled_instance = instance * scaling_factor\n",
        "\n",
        "        # Add augmented instances to list\n",
        "        augmented_data.extend([rotated_instance, translated_instance, scaled_instance])\n",
        "\n",
        "    return np.array(augmented_data)\n",
        "\n",
        "# Define function to rotate instance\n",
        "def rotate_instance(instance, angle):\n",
        "    rotated_instance = np.roll(instance, int(angle))  # Roll the array by the angle\n",
        "    return rotated_instance\n",
        "\n",
        "# Assuming you have X_train and Y_train datasets\n",
        "# Apply geometric transformations to training data\n",
        "X_train_augmented = apply_geometric_transformations(X_train)\n",
        "Y_train_augmented = apply_geometric_transformations(Y_train)\n",
        "\n",
        "# Trim Y_train_augmented to match the number of samples in X_train_augmented\n",
        "num_samples_to_keep = X_train_augmented.shape[0]\n",
        "Y_train_augmented_trimmed = Y_train_augmented[:num_samples_to_keep]\n",
        "\n",
        "print(\"Shape of X_train_augmented:\", X_train_augmented.shape)\n",
        "print(\"Shape of Y_train_augmented_trimmed:\", Y_train_augmented_trimmed.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi2CBQa1ggaB"
      },
      "source": [
        "Step No 3: Transformer Based Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mITkvO20gjlJ",
        "outputId": "61f6c26c-3b8c-403b-a669-e35787af2959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Transformer_Autoencoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)        [(None, 25)]                 0         []                            \n",
            "                                                                                                  \n",
            " reshape_5 (Reshape)         (None, 25, 1)                0         ['input_8[0][0]']             \n",
            "                                                                                                  \n",
            " time_distributed_8 (TimeDi  (None, 25, 128)              256       ['reshape_5[0][0]']           \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " permute_8 (Permute)         (None, 128, 25)              0         ['time_distributed_8[0][0]']  \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (Mu  (None, 128, 25)              105497    ['permute_8[0][0]',           \n",
            " ltiHeadAttention)                                                   'permute_8[0][0]',           \n",
            "                                                                     'permute_8[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_4 (Lay  (None, 128, 25)              50        ['multi_head_attention_4[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " permute_9 (Permute)         (None, 25, 128)              0         ['layer_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " time_distributed_9 (TimeDi  (None, 25, 128)              16512     ['permute_9[0][0]']           \n",
            " stributed)                                                                                       \n",
            "                                                                                                  \n",
            " time_distributed_10 (TimeD  (None, 25, 128)              16512     ['time_distributed_9[0][0]']  \n",
            " istributed)                                                                                      \n",
            "                                                                                                  \n",
            " permute_10 (Permute)        (None, 128, 25)              0         ['time_distributed_10[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (Mu  (None, 128, 25)              105497    ['permute_10[0][0]',          \n",
            " ltiHeadAttention)                                                   'permute_10[0][0]',          \n",
            "                                                                     'permute_10[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_5 (Lay  (None, 128, 25)              50        ['multi_head_attention_5[0][0]\n",
            " erNormalization)                                                   ']                            \n",
            "                                                                                                  \n",
            " permute_11 (Permute)        (None, 25, 128)              0         ['layer_normalization_5[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " time_distributed_11 (TimeD  (None, 25, 1)                129       ['permute_11[0][0]']          \n",
            " istributed)                                                                                      \n",
            "                                                                                                  \n",
            " reshape_6 (Reshape)         (None, 25)                   0         ['time_distributed_11[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 244503 (955.09 KB)\n",
            "Trainable params: 244503 (955.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Reshape, Permute, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define Transformer-based Autoencoder model\n",
        "def transformer_autoencoder(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    encoder = Reshape((input_shape[0], 1))(inputs)\n",
        "    encoder = TimeDistributed(Dense(128, activation='relu'))(encoder)  # Time-distributed dense layer\n",
        "    encoder = Permute((2, 1))(encoder)  # Permute the dimensions to (batch_size, features, time_steps)\n",
        "    encoder = MultiHeadAttention(num_heads=8, key_dim=128)(encoder, encoder, encoder)    # Multi-head self-attention layer\n",
        "    encoder = LayerNormalization(epsilon=1e-6)(encoder)                # Layer normalization\n",
        "    encoder = Permute((2, 1))(encoder)  # Permute back to (batch_size, time_steps, features)\n",
        "    encoder = TimeDistributed(Dense(128, activation='relu'))(encoder)  # Time-distributed dense layer\n",
        "\n",
        "    # Decoder\n",
        "    decoder = TimeDistributed(Dense(128, activation='relu'))(encoder)   # Time-distributed dense layer\n",
        "    decoder = Permute((2, 1))(decoder)  # Permute the dimensions\n",
        "    decoder = MultiHeadAttention(num_heads=8, key_dim=128)(decoder, decoder, decoder)     # Multi-head self-attention layer\n",
        "    decoder = LayerNormalization(epsilon=1e-6)(decoder)                 # Layer normalization\n",
        "    decoder = Permute((2, 1))(decoder)  # Permute back\n",
        "    decoder = TimeDistributed(Dense(1, activation='relu'))(decoder)  # Time-distributed dense layer\n",
        "\n",
        "    # Reshape back to original shape\n",
        "    decoder = Reshape((input_shape[0],))(decoder)\n",
        "\n",
        "    # Define autoencoder model\n",
        "    autoencoder = Model(inputs, decoder, name=\"Transformer_Autoencoder\")\n",
        "    return autoencoder\n",
        "\n",
        "# Define input shape (assuming time series data)\n",
        "input_shape = X_train_augmented.shape[1:]\n",
        "\n",
        "# Create Transformer-based Autoencoder model\n",
        "autoencoder = transformer_autoencoder(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Print model summary\n",
        "autoencoder.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuWH5EKeh151"
      },
      "source": [
        "Step No 4: Constrastic Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--xpeif1h5bl",
        "outputId": "56a493a8-b5a5-40fa-bf5c-f1066f2ab231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Contrastive_Autoencoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 25)]              0         \n",
            "                                                                 \n",
            " Transformer_Autoencoder (F  (None, 25)                244503    \n",
            " unctional)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 244503 (955.09 KB)\n",
            "Trainable params: 244503 (955.09 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Reshape, Permute\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define custom contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred, margin=1):\n",
        "    # Euclidean distance between the true and predicted representations\n",
        "    euclidean_distance = tf.sqrt(tf.reduce_sum(tf.square(y_true - y_pred), axis=-1))\n",
        "    # Contrastive loss\n",
        "    return tf.reduce_mean((1 - y_true) * tf.square(euclidean_distance) + y_true * tf.square(tf.maximum(margin - euclidean_distance, 0)))\n",
        "\n",
        "# Load your existing encoder model\n",
        "encoder_model = transformer_autoencoder(input_shape)\n",
        "\n",
        "# Define input layer for contrastive learning\n",
        "contrastive_input = Input(shape=input_shape)\n",
        "\n",
        "# Get the encoded representation using the existing encoder model\n",
        "encoded_representation = encoder_model(contrastive_input)\n",
        "\n",
        "# Define contrastive autoencoder model\n",
        "contrastive_autoencoder = Model(contrastive_input, encoded_representation, name=\"Contrastive_Autoencoder\")\n",
        "\n",
        "# Compile the model with contrastive loss\n",
        "contrastive_autoencoder.compile(optimizer='adam', loss=contrastive_loss)\n",
        "\n",
        "# Print model summary\n",
        "contrastive_autoencoder.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tiA7uE7iqBh"
      },
      "source": [
        "Step No 5 : Integrating GAN into This Archietcture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2_03czFiusu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Reshape, Permute\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import BinaryAccuracy\n",
        "\n",
        "# Define Generator model\n",
        "def build_generator(latent_dim, output_shape):\n",
        "    inputs = Input(shape=(latent_dim,))\n",
        "    x = Dense(128, activation='relu')(inputs)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(output_shape[0], activation='sigmoid')(x)  # Assuming output_shape[0] is the number of features\n",
        "    generator = Model(inputs, outputs, name='Generator')\n",
        "    return generator\n",
        "\n",
        "# Define Discriminator model\n",
        "def build_discriminator(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Dense(128, activation='relu')(inputs)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    discriminator = Model(inputs, outputs, name='Discriminator')\n",
        "    return discriminator\n",
        "\n",
        "# Define GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(shape=(latent_dim,))\n",
        "    gan_output = discriminator(generator(gan_input))\n",
        "    gan = Model(gan_input, gan_output, name='GAN')\n",
        "    return gan\n",
        "\n",
        "# Define input shape (assuming time series data)\n",
        "input_shape = X_train_augmented.shape[1:]\n",
        "\n",
        "# Define latent dimension for generator\n",
        "latent_dim = 100  # You can adjust this as needed\n",
        "\n",
        "# Build Generator and Discriminator\n",
        "generator = build_generator(latent_dim, input_shape)\n",
        "discriminator = build_discriminator(input_shape)\n",
        "\n",
        "# Build GAN\n",
        "gan = build_gan(generator, discriminator)\n",
        "\n",
        "# Compile GAN\n",
        "gan.compile(optimizer=Adam(), loss=BinaryCrossentropy(), metrics=[BinaryAccuracy()])\n",
        "\n",
        "# Train GAN (while training autoencoder)\n",
        "# You need to write your training loop here, where you generate fake samples using the generator,\n",
        "# combine them with real samples, and train the discriminator to distinguish between real and fake samples.\n",
        "# Then, you train the GAN to generate more realistic samples while keeping the discriminator fixed.\n",
        "\n",
        "# After training the GAN, you can use the generator to generate synthetic data samples.\n",
        "\n",
        "# Example:\n",
        "# for epoch in range(epochs):\n",
        "#     for batch in dataset:\n",
        "#         # Train Discriminator\n",
        "#         real_samples = batch\n",
        "#         fake_samples = generator(tf.random.normal((batch_size, latent_dim)))\n",
        "#         discriminator.train_on_batch(real_samples, tf.ones((batch_size, 1)))\n",
        "#         discriminator.train_on_batch(fake_samples, tf.zeros((batch_size, 1)))\n",
        "#\n",
        "#         # Train Generator (GAN)\n",
        "#         gan.train_on_batch(tf.random.normal((batch_size, latent_dim)), tf.ones((batch_size, 1)))\n",
        "\n",
        "# Now you can use the trained generator to generate synthetic data samples.\n",
        "\n",
        "# Example:\n",
        "# synthetic_samples = generator(tf.random.normal((num_samples, latent_dim)))\n",
        "\n",
        "# Now you can proceed with training your autoencoder using the synthetic samples along with real samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6bvLlwIjDlp"
      },
      "source": [
        "Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxnjfthijFNi",
        "outputId": "f6f19a27-46d7-4f2d-c376-cbd55ff6e516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1033/1033 [==============================] - 4s 3ms/step - loss: 0.7162 - val_loss: 0.6814\n",
            "Epoch 2/10\n",
            "1033/1033 [==============================] - 3s 3ms/step - loss: 0.6045 - val_loss: 0.6490\n",
            "Epoch 3/10\n",
            "1033/1033 [==============================] - 2s 2ms/step - loss: 0.5889 - val_loss: 0.6431\n",
            "Epoch 4/10\n",
            "1033/1033 [==============================] - 3s 2ms/step - loss: 0.5855 - val_loss: 0.6412\n",
            "Epoch 5/10\n",
            "1033/1033 [==============================] - 2s 2ms/step - loss: 0.5841 - val_loss: 0.6402\n",
            "Epoch 6/10\n",
            "1033/1033 [==============================] - 3s 3ms/step - loss: 0.5833 - val_loss: 0.6396\n",
            "Epoch 7/10\n",
            "1033/1033 [==============================] - 3s 3ms/step - loss: 0.5829 - val_loss: 0.6392\n",
            "Epoch 8/10\n",
            "1033/1033 [==============================] - 2s 2ms/step - loss: 0.5825 - val_loss: 0.6390\n",
            "Epoch 9/10\n",
            "1033/1033 [==============================] - 2s 2ms/step - loss: 0.5823 - val_loss: 0.6388\n",
            "Epoch 10/10\n",
            "1033/1033 [==============================] - 2s 2ms/step - loss: 0.5822 - val_loss: 0.6387\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# Define your model architecture\n",
        "input_dim = X_train_augmented.shape[1]  # Assuming the input dimension is the number of features\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoded = Dense(64, activation='relu')(input_layer)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "model = Model(input_layer, decoded)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Randomly sample a subset of X_train_augmented to match the size of Y_train_augmented_trimmed\n",
        "subset_indices = np.random.choice(X_train_augmented.shape[0], size=Y_train_augmented_trimmed.shape[0], replace=False)\n",
        "X_train_subset = X_train_augmented[subset_indices]\n",
        "\n",
        "# Split the augmented training set into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_subset, Y_train_augmented_trimmed, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model using the training set\n",
        "history = model.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=10, batch_size=32)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the true labels from the test_label.csv file\n",
        "true_labels = pd.read_csv('test_label.csv')\n",
        "\n",
        "# Assuming 'label' column contains the true labels (0s and 1s)\n",
        "y_test = true_labels['label']\n",
        "\n",
        "# Ensure that y_test contains binary labels (0s and 1s)\n",
        "# Convert multiclass labels to binary labels if necessary\n",
        "# For example:\n",
        "# y_test_binary = (y_test != 'normal').astype(int)  # Convert 'normal' class to 0, others to 1\n",
        "\n",
        "# Ensure that the lengths of y_test and y_pred_binary are the same\n",
        "y_test = y_test[:len(y_pred_binary)]\n",
        "\n",
        "print(y_test)\n",
        "\n",
        "print(y_test_binary)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOE0AoP-T7gN",
        "outputId": "64d45503-7ec7-4034-e520-2815677e8574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "17201    0\n",
            "17202    0\n",
            "17203    0\n",
            "17204    0\n",
            "17205    0\n",
            "Name: label, Length: 17206, dtype: int64\n",
            "0        1\n",
            "1        1\n",
            "2        1\n",
            "3        1\n",
            "4        1\n",
            "        ..\n",
            "87836    1\n",
            "87837    1\n",
            "87838    1\n",
            "87839    1\n",
            "87840    1\n",
            "Name: label, Length: 87841, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last Step : Hyper Parameter Tuning"
      ],
      "metadata": {
        "id": "SVduJsbuWuSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define your model architecture\n",
        "def create_model(learning_rate=0.01, dropout_rate=0.2):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "        Dropout(dropout_rate),\n",
        "        BatchNormalization(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(dropout_rate),\n",
        "        BatchNormalization(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Create a function to build and compile the model\n",
        "def build_fn(learning_rate=0.01, dropout_rate=0.2):\n",
        "    model = create_model(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9hS7QerRWzzc",
        "outputId": "423dd274-0bd5-44fc-e307-7e1999bcb80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow.keras' has no attribute 'wrappers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-949c9a010c3a>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Perform grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m grid = GridSearchCV(estimator=tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_fn, epochs=10, batch_size=32, verbose=0),\n\u001b[0m\u001b[1;32m     37\u001b[0m                     param_grid=param_grid, cv=3)\n\u001b[1;32m     38\u001b[0m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'wrappers'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}